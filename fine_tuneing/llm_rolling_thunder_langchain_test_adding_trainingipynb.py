# -*- coding: utf-8 -*-
"""llm_rolling_thunder_langchain_test_adding_trainingipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VHsFL5VjdmJHODRfughY4nSkAWz2jgCE



Causal language modeling: the model has to predict the next token in the sentence (so the labels are the same as the inputs shifted to 
the right). To make sure the model does not cheat, 
it gets an attention mask that will prevent it to access the tokens after token i when trying to predict the token i+1 in the sentence.

"""

# DAVID VAUGHAN R 1166390,Roman Tait, Tucker Hoffnagle,Karlton Hall,Harrison Whitworth
# Rolling Thunder
# LLM for  Lockheed Martin

"""https://github.com/davivaug2/LLM_RollingThunder



# Next code Section is downloading all dependcies for Google Colab. Not Neccesarry on HPCC
"""

"""# Next code Section is importing depency on python
"""

from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
# from langchain.embeddings import HuggingFaceEmbeddings
 # changed
from langchain.embeddings import (
    #LlamaCppEmbeddings,
    HuggingFaceEmbeddings,
    SentenceTransformerEmbeddings
)

from langchain.chains import RetrievalQA
from langchain.document_loaders import UnstructuredFileLoader
from langchain.document_loaders.pdf import UnstructuredPDFLoader
 # changed
from langchain.document_loaders import (
    PyPDFLoader,
    DataFrameLoader,
    GitLoader
  )
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQAWithSourcesChain
from huggingface_hub import notebook_login
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain import HuggingFacePipeline
from langchain.text_splitter import CharacterTextSplitter

import textwrap
import sys
import torch # pipe
import textwrap
import os
NOT_DOWNLOADED_PDF = False
NODE = 1
cpu_per_node = 40
NUM_PROC = NODE * cpu_per_node

# for importing LLM from Huggingface
hug_write_toke = "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"
os.environ['HuggingFaceHub_API_Token']= hug_write_toke
from huggingface_hub import login

login(hug_write_toke)

"""# Next code Section is importing the PDF.Got some of the code from https://dev.to/seraph776/download-pdf-files-using-python-4064"""

import requests
def download_pdf_file(url: str) -> bool:
    """Download PDF from given URL to local directory.

    :param url: The url of the PDF file to be downloaded
    :return: True if PDF file was successfully downloaded, otherwise False.
    """

    # Request URL and get response object
    response = requests.get(url, stream=True)

    # isolate PDF filename from URL
    pdf_file_name = os.path.basename(url)
    if response.status_code == 200:
        # Save in current working directory
        filepath = os.path.join(os.getcwd(), pdf_file_name)
        with open(filepath, 'wb') as pdf_object:
            pdf_object.write(response.content)
            print(f'{pdf_file_name} was successfully saved!')
            return True
    else:
        print(f'Uh oh! Could not download {pdf_file_name},')
        print(f'HTTP response status code: {response.status_code}')
        return False

# RAW FILE
# https://githubraw.com/
pf1 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-20-1_1.pdf'
pf2 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-20-2.pdf'
pf3 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-20-3.PDF'
pf4 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-20-9.pdf'
pf5 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-25-107.pdf'
pf6 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-25-195.pdf'
pf7 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-25-4.pdf'
pf8 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-35A-39_1.PDF'
pf9 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-35D-54.pdf'
pf10 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-5-15.PDF'
pf11 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-5-18.pdf'
pf12 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-5-19.pdf'
pf13 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-5-1_1.PDF'
pf14 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-5-3.pdf'
pf15 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/1-1-300.PDF'
pf16 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/33-1-37-2.pdf'
pf17 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/33-1-37-3.pdf'
pf18 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/33-1-37-4.pdf'
pf19 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/33B-1-1_R14-compressed.pdf'
pf20 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-021019-1-1B-50.pdf'
pf21 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-082216-00-80G-1.pdf'
pf22 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-082416-00-85B-3.pdf'
pf23 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-082416-1-1A-1.pdf'
pf24 ='https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-082516-33-1-37-1.pdf'
pf25 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-112818-00-85A-03-1.pdf'
pf26 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-131009-054.PDF'
pf27 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-191106-00-5-16.pdf'
list_pdf = {pf1,pf2,pf3,pf4,pf5,pf6,pf7,pf8,pf9,pf10,pf11,pf12,pf13,pf14,pf15,pf16,pf17,pf18,pf19,pf20,pf21
            ,pf22,pf23,pf24,pf25,pf26,pf27}
if(NOT_DOWNLOADED_PDF == True):
   for pd in list_pdf:
      download_pdf_file(pd)


name1 = '00-20-1_1.pdf'
name2 = '00-20-2.pdf'
name3 = '00-20-3.PDF'
name4 = '00-20-9.pdf'
name5 = '00-25-107.pdf'
name6 = '00-25-195.pdf'
name7 = '00-25-4.pdf'
name8 = '00-35A-39_1.PDF'
name9 = '00-35D-54.pdf'
name10 = '00-5-15.PDF'
name11 = '00-5-18.pdf'
name12 = '00-5-19.pdf'
name13 = '00-5-1_1.PDF'
name14 = '00-5-3.pdf'
name15 = '1-1-300.PDF'
name16 = '33-1-37-2.pdf'
name17 = '33-1-37-3.pdf'
name19 = '33-1-37-4.pdf'
name19 = '33B-1-1_R14-compressed.pdf'
name20 = 'AFD-021019-1-1B-50.pdf'
name21 = 'AFD-082216-00-80G-1.pdf'
name22 = 'AFD-082416-00-85B-3.pdf'
name23 = 'AFD-082416-1-1A-1.pdf'
name24 = 'AFD-082516-33-1-37-1.pdf'
name25 = 'AFD-112818-00-85A-03-1.pdf'
name26 = 'AFD-131009-054.PDF'
name27 = 'AFD-191106-00-5-16.pdf'
list_name = {name1,name2,name3,name4,name5,name6,name7 ,name8,name9,name10,name11,name12 ,
             name13 ,name14 ,name15 ,name16,name17 ,name19 ,name19 ,name20 ,name21 ,name22,name23,name24,name25 ,name26 ,name27}
# list_name = {name1}
             

"""# Next code Section is for takeing the PDF's and vectorize them for a database. Got some of the code from https://colab.research.google.com/drive/1Z9R5wSF9tF_4bxYSXVKmOHM2gu7B7QVQ#scrollTo=NvEsaUTrReEG"""


def get_pdf_splits(pdf_file):
  """Function takes in the pdf data and returns the
  splits so for further processing can be done."""

  loader = PyPDFLoader(pdf_file)
  pages = loader.load_and_split()

  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,
                                             chunk_overlap=15,
                                             length_function=len)
  doc_list = []
  #Pages will be list of pages, so need to modify the loop
  for pg in pages:
    pg_splits = textSplit.split_text(pg.page_content)
    doc_list.extend(pg_splits)

  return doc_list
not_added = False
all_doc = []
if(not_added == True):
  for name in list_name:
    pdf_docs = get_pdf_splits(name)
    all_doc.append(pdf_docs)

flat_list = [item for sublist in all_doc for item in sublist]
from datasets import load_dataset
from peft import LoraConfig
#pdf_docs = get_pdf_splits("merged.pdf")
# add all document chinks
not_added = False
all_doc = []
if(not_added == True):
  for name in list_name:
    pdf_docs = get_pdf_splits(name)
    all_doc.append(pdf_docs)

flat_list = [item for sublist in all_doc for item in sublist]
from datasets import load_dataset

import random
random.shuffle(flat_list)
#chunk_size = 600
#chunked_list = [flat_list[i:i+chunk_size] for i in range(0, len(flat_list), chunk_size)]
#print(len(chunked_list))
length = len(flat_list) // 1
#length = len(flat_list) // 5
train_set, test_set = flat_list[:length], flat_list[length:]
not_added2 = False
print(len(test_set))
print(len(train_set))
if(not_added2 == True):
  print(len(train_set))
  with open('train.txt', 'w') as outfile:
    outfile.write('\n'.join(str(i) for i in train_set))
  outfile.close()
  print(len(test_set))
  with open('test.txt', 'w') as outfile:
    outfile.write('\n'.join(str(i) for i in test_set))
  outfile.close()
###########################
# load dataset
#dataset = load_dataset('text', data_files={'train': ['train.txt'], 'validation': 'test.txt'})
#dataset2 = load_dataset2(dataset_name, split="train")
dataset2 = load_dataset('text', data_files={'text': ['train.txt']})
block_size = 128
train_dataset2 = dataset2['text']
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import transformers
import torch

model_name = "vilsonrodrigues/falcon-7b-instruct-sharded"
#use_flash_attention_2=True,
# torch_dtype=torch.float32, 
model = AutoModelForCausalLM.from_pretrained(model_name,
    torch_dtype=torch.float32, 
    device_map = 'auto' ,trust_remote_code=True) #LLM Model
tokenizer = AutoTokenizer.from_pretrained(model_name)
# necessary for training to work for some reason
#tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.pad_token = tokenizer.eos_token

#model.config.use_cache = False
from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training


# peft_config adds peft paramaters to the model for training.
#model = prepare_model_for_kbit_training(model)
lora_alpha = 32 # scaling factor for the weight matrices
lora_dropout = 0.05 # dropout probability of the LoRA layers
lora_rank = 32 # dimension of the low-rank matrices
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_rank,
    bias="none",  # setting to 'none' for only training weight params instead of biases
    task_type="CAUSAL_LM",
    target_modules=[         # Setting names of modules in falcon-7b model that we want to apply LoRA to
        "query_key_value",
        "dense",
        "dense_h_to_4h",
        "dense_4h_to_h",
    ]
)

def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}"
    )




# Tokenize the dataset
#def tokenize_function(examples):
#    return tokenizer(examples["text"], truncation=True)
#train_dataset2_tok = train_dataset2.map(tokenize_function, batched=True)
from transformers import DataCollatorWithPadding
from torch.nn.parallel import DataParallel
from transformers import AutoModelForCausalLM ,BitsAndBytesConfig
from transformers import Trainer, TrainingArguments
# TrainingArguments
output_dir = "./falcon-7b-instruct-sharded_large_peft_train"
output_dir_name = "falcon-7b-instruct-sharded_large_peft_train"
per_device_train_batch_size = 4
gradient_accumulation_steps = 4
optim = "paged_adamw_32bit"
save_steps = 10
logging_steps = 10
learning_rate = 2e-4
max_grad_norm = 0.3
max_steps = 10000
warmup_ratio = 0.03
lr_scheduler_type = "constant"
push_to_hub_model_id="jellyconsumer/falcon-7b-instruct-sharded_large_peft_train"
#torch.float32
#    output_dir=output_dir,
#save_strategy = "no",
# gradient_checkpointing=False
# evaluation_strategy = "epoch",
training_arguments = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir =  True,
   # evaluation_strategy = "epoch",
    save_strategy = "no",
    num_train_epochs=1,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=True,
    lr_scheduler_type=lr_scheduler_type,
    gradient_checkpointing=True,
    load_best_model_at_end=True,
    push_to_hub=True,
    hub_model_id=push_to_hub_model_id,
    evaluation_strategy="no" ,
    do_eval=False,
)
from transformers import TextDataset, DataCollatorForLanguageModeling
from trl import SFTTrainer
from transformers import Trainer
max_seq_length = 512
#    dataset_text_field="text",
#  do not train model parameters
for param in model.parameters():
    # freeze base model's layers
    param.requires_grad = False
# make PEFT paramters trainaible I think
if hasattr(model, "enable_input_require_grads"):
    model.enable_input_require_grads()
else:
    def make_inputs_require_grad(module, input, output):
        output.requires_grad_(True)

    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)

# SFTTrainer
peft_model = get_peft_model(model, peft_config)
peft_model.config.use_cache = False
#model.config.use_cache = False
if hasattr(model, "enable_input_require_grads"):
    model.enable_input_require_grads()
else:
    def make_inputs_require_grad(module, input, output):
         output.requires_grad_(True)

    model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)

trainer = SFTTrainer(
    model=model,
    args=training_arguments,
    train_dataset=train_dataset2,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer
    
)
#for name, module in trainer.model.named_modules():
#    if "norm" in name:
#        module = module.to(torch.float32)
print("Start training")
trainer.train()
print("DONE training")
trainer.push_to_hub()
print("DONE")
