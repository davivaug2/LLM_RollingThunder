# -*- coding: utf-8 -*-
"""llm_rolling_thunder_langchain_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VfVMSvrKAmlgOcc9yOjBNC-TITuZZhpc
"""



import os
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
# from langchain.embeddings import HuggingFaceEmbeddings
 # changed
from langchain.embeddings import (
    #LlamaCppEmbeddings,
    HuggingFaceEmbeddings,
    SentenceTransformerEmbeddings
)

from langchain.chains import RetrievalQA
from langchain.document_loaders import UnstructuredFileLoader
from langchain.document_loaders.pdf import UnstructuredPDFLoader
 # changed
from langchain.document_loaders import (
    PyPDFLoader,
    DataFrameLoader,
    GitLoader
  )
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQAWithSourcesChain
from huggingface_hub import notebook_login
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain import HuggingFacePipeline
from langchain.text_splitter import CharacterTextSplitter
import textwrap
import sys

import torch # pipe

import textwrap

hug_write_toke = "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"

os.environ['HuggingFaceHub_API_Token']= hug_write_toke

import requests
def download_pdf_file(url: str) -> bool:
    """Download PDF from given URL to local directory.

    :param url: The url of the PDF file to be downloaded
    :return: True if PDF file was successfully downloaded, otherwise False.
    """

    # Request URL and get response object
    response = requests.get(url, stream=True)

    # isolate PDF filename from URL
    pdf_file_name = os.path.basename(url)
    if response.status_code == 200:
        # Save in current working directory
        filepath = os.path.join(os.getcwd(), pdf_file_name)
        with open(filepath, 'wb') as pdf_object:
            pdf_object.write(response.content)
            print(f'{pdf_file_name} was successfully saved!')
            return True
    else:
        print(f'Uh oh! Could not download {pdf_file_name},')
        print(f'HTTP response status code: {response.status_code}')
        return False

# RAW FILE
# https://githubraw.com/
pf1 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-20-1_1.pdf'
pf2 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-20-2.pdf'
pf3 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-20-3.PDF'
pf4 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-20-9.pdf'
pf5 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-25-107.pdf'
pf6 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-25-195.pdf'
pf7 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-25-4.pdf'
pf8 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-35A-39_1.PDF'
pf9 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-35D-54.pdf'
pf10 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-5-15.PDF'
pf11 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-5-18.pdf'
pf12 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-5-19.pdf'
pf13 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-5-1_1.PDF'
pf14 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/00-5-3.pdf'
pf15 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/1-1-300.PDF'
pf16 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/33-1-37-2.pdf'
pf17 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/33-1-37-3.pdf'
pf18 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/33-1-37-4.pdf'
pf19 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/33B-1-1_R14-compressed.pdf'
pf20 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-021019-1-1B-50.pdf'
pf21 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-082216-00-80G-1.pdf'
pf22 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-082416-00-85B-3.pdf'
pf23 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-082416-1-1A-1.pdf'
pf24 ='https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-082516-33-1-37-1.pdf'
pf25 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-112818-00-85A-03-1.pdf'
pf26 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-131009-054.PDF'
pf27 = 'https://cdn.githubraw.com/davivaug2/LLM_RollingThunder/main/folder1/AFD-191106-00-5-16.pdf'
list_pdf = {pf1,pf2,pf3,pf4,pf5,pf6,pf7,pf8,pf9,pf10,pf11,pf12,pf13,pf14,pf15,pf16,pf17,pf18,pf19,pf20,pf21
            ,pf22,pf23,pf24,pf25,pf26,pf27}

for pd in list_pdf:
  download_pdf_file(pd)

name1 = '00-20-1_1.pdf'
name2 = '00-20-2.pdf'
name3 = '00-20-3.PDF'
name4 = '00-20-9.pdf'
name5 = '00-25-107.pdf'
name6 = '00-25-195.pdf'
name7 = '00-25-4.pdf'
name8 = '00-35A-39_1.PDF'
name9 = '00-35D-54.pdf'
name10 = '00-5-15.PDF'
name11 = '00-5-18.pdf'
name12 = '00-5-19.pdf'
name13 = '00-5-1_1.PDF'
name14 = '00-5-3.pdf'
name15 = '1-1-300.PDF'
name16 = '33-1-37-2.pdf'
name17 = '33-1-37-3.pdf'
name19 = '33-1-37-4.pdf'
name19 = '33B-1-1_R14-compressed.pdf'
name20 = 'AFD-021019-1-1B-50.pdf'
name21 = 'AFD-082216-00-80G-1.pdf'
name22 = 'AFD-082416-00-85B-3.pdf'
name23 = 'AFD-082416-1-1A-1.pdf'
name24 = 'AFD-082516-33-1-37-1.pdf'
name25 = 'AFD-112818-00-85A-03-1.pdf'
name26 = 'AFD-131009-054.PDF'
name27 = 'AFD-191106-00-5-16.pdf'
list_name = {name1,name2,name3,name4,name5,name6,name7 ,name8,name9,name10,name11,name12 ,
             name13 ,name14 ,name15 ,name16,name17 ,name19 ,name19 ,name20 ,name21 ,name22,name23,name24,name25 ,name26 ,name27}

def get_text_splits(text_file):
  """Function takes in the text data and returns the
  splits so for further processing can be done."""
  with open(text_file,'r') as txt:
    data = txt.read()

  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,
                                             chunk_overlap=15,
                                             length_function=len)
  doc_list = textSplit.split_text(data)
  return doc_list

def get_pdf_splits(pdf_file):
  """Function takes in the pdf data and returns the
  splits so for further processing can be done."""

  loader = PyPDFLoader(pdf_file)
  pages = loader.load_and_split()

  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,
                                             chunk_overlap=15,
                                             length_function=len)
  doc_list = []
  #Pages will be list of pages, so need to modify the loop
  for pg in pages:
    pg_splits = textSplit.split_text(pg.page_content)
    doc_list.extend(pg_splits)

  return doc_list

'''
## try this out in the Fututre
def get_git_files(repo_link, folder_path, file_ext):
  # eg. loading only python files
  git_loader = GitLoader(clone_url=repo_link,
    repo_path=folder_path,
    file_filter=lambda file_path: file_path.endswith(file_ext))
  #Will take each file individual document
  git_docs = git_loader.load()

  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,
                                             chunk_overlap=15,
                                             length_function=len)
  doc_list = []
  #Pages will be list of pages, so need to modify the loop
  for code in git_docs:
    code_splits = textSplit.split_text(code.page_content)
    doc_list.extend(code_splits)

  return doc_list
  '''

def embed_index(doc_list, embed_fn, index_store):
  """Function takes in existing vector_store,
  new doc_list and embedding function that is
  initialized on appropriate model. Local or online.
  New embedding is merged with the existing index. If no
  index given a new one is created"""
  #check whether the doc_list is documents, or text
  try:
    faiss_db = FAISS.from_documents(doc_list,
                              embed_fn)
  except Exception as e:
    faiss_db = FAISS.from_texts(doc_list,
                              embed_fn)

  if os.path.exists(index_store):
    local_db = FAISS.load_local(index_store,embed_fn)
    #merging the new embedding with the existing index store
    local_db.merge_from(faiss_db)
    print("Merge completed")
    local_db.save_local(index_store)
    print("Updated index saved")
  else:
    faiss_db.save_local(folder_path=index_store)
    print("New store created...")

def get_docs_length(index_path, embed_fn):
  test_index = FAISS.load_local(index_path,
                              embeddings=embed_fn)
  test_dict = test_index.docstore._dict
  return len(test_dict.values())

embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',model_kwargs={'device': 'cuda'})

for name in list_name:
  pdf_docs = get_pdf_splits(name)
  embed_index(doc_list=pdf_docs,
            embed_fn=embeddings,
            index_store='new_index')

# manually upload odf
#loader = UnstructuredPDFLoader('33B-1-1_R14-compressed.pdf')
#documents = loader.load()

'''
text_splitter=CharacterTextSplitter(separator='\n',
                                    chunk_size=1000,
                                    chunk_overlap=50)
text_chunks=text_splitter.split_documents(documents)
'''

#vectorstore=FAISS.from_documents(text_chunks, embeddings)
if( os.path.exists('new_index')):
  vectorstore = FAISS.load_local(folder_path='new_index',
                                embeddings=embeddings)



model = "tiiuae/falcon-7b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model)

pipe = pipeline("text-generation",
                model=model,
                tokenizer= tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                max_new_tokens = 1024,
                do_sample=True,
                top_k=10,
                num_return_sequences=1,
                eos_token_id=tokenizer.eos_token_id
                )

llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})

chain =  RetrievalQA.from_chain_type(llm=llm, chain_type = "stuff",return_source_documents=True, retriever=vectorstore.as_retriever())

query = "What is Liquid Penetrant Inspection?"
result=chain({"query": query}, return_only_outputs=True)
wrapped_text = textwrap.fill(result['result'], width=500)
wrapped_text

wrapper = textwrap.TextWrapper(width=76)
word_list = wrapper.wrap(text=wrapped_text)
# Print each line.
for element in word_list:
    print(element)
print(result['source_documents'])

print(wrapped_text)
print(result['source_documents'])

query2 = " How can I be happy?"
result2=chain({"query": query2}, return_only_outputs=True)
wrapped_text2 = textwrap.fill(result2['result'], width=500)
wrapped_text2

print(wrapped_text2)
print(result2['source_documents'])




query3 = " What is 36A12-13-18-1?"
result3=chain({"query": query3}, return_only_outputs=True)
wrapped_text3 = textwrap.fill(result3['result'], width=500)
wrapped_text3

print(wrapped_text3)
print(result3['source_documents'])

file1 = open("Output.txt", "w")
#file1.write(wrapped_text2+'\n'+"ENDING")
file1.write(wrapped_text+'\n'+wrapped_text2+'\n'+wrapped_text3)
file1.close()
print("DONE WITH PROGRAM")